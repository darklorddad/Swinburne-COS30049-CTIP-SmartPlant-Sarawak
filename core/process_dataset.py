import os
import shutil
import random
import pathlib
from PIL import Image, ImageFile

def process_dataset(source_dir, dest_dir, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, resolution=None, seed=None, progress_callback=None, cancel_event=None, image_extensions=None, color_mode='RGB', overwrite_dest=False, load_truncated_images=True, train_dir_name='train', val_dir_name='val', test_dir_name='test'):
    """
    Processes an image dataset by splitting it into training, validation, and test sets

    Recursively scans the source_dir to find subdirectories containing image files,
    and treats each such directory as a class

    Args:
        source_dir (str): The directory containing the raw dataset
        dest_dir (str): The directory where the processed 'train', 'val', and 'test' sets will be saved
        train_ratio (float): The proportion of images for training
        val_ratio (float): The proportion of images for validation
        test_ratio (float): The proportion of images for testing
        resolution (int, optional): The resolution to resize images to If None, images are not resized
        progress_callback (function, optional): A function to call with progress updates
        image_extensions (str, optional): Comma-separated string of image extensions to include.
        color_mode (str, optional): The color mode to convert images to (e.g., 'RGB', 'L').
    """
    def log(message):
        print(message)
        if progress_callback:
            progress_callback(message)

    ImageFile.LOAD_TRUNCATED_IMAGES = load_truncated_images
    log("Starting dataset processing")

    if seed is not None:
        random.seed(seed)
        log(f"Using random seed: {seed}")

    source_path = pathlib.Path(source_dir)
    dest_path = pathlib.Path(dest_dir)
    if image_extensions:
        image_extensions = {ext.strip().lower() for ext in image_extensions.split(',')}
    else:
        image_extensions = {'.jpg', '.jpeg', '.png'}

    # Ensure the destination directory exists
    dest_path.mkdir(parents=True, exist_ok=True)
    train_dest_path = dest_path / train_dir_name
    val_dest_path = dest_path / val_dir_name
    test_dest_path = dest_path / test_dir_name

    # Check if destination directories already contain data.
    if (train_dest_path.exists() and any(train_dest_path.iterdir())) or \
       (val_dest_path.exists() and any(val_dest_path.iterdir())) or \
       (test_dest_path.exists() and any(test_dest_path.iterdir())):
        if overwrite_dest:
            log("Destination directory is not empty. Overwriting...")
            if train_dest_path.exists():
                shutil.rmtree(train_dest_path)
            if val_dest_path.exists():
                shutil.rmtree(val_dest_path)
            if test_dest_path.exists():
                shutil.rmtree(test_dest_path)
        else:
            log("Destination directory is not empty. Please clear it first or enable 'Overwrite destination'")
            return

    train_dest_path.mkdir(exist_ok=True)
    val_dest_path.mkdir(exist_ok=True)
    if test_ratio > 0:
        test_dest_path.mkdir(exist_ok=True)

    # Recursively scan for directories with images.
    all_dirs = [d for d in source_path.rglob('*') if d.is_dir()]
    class_dirs = []
    for d in all_dirs:
        if any(f.suffix.lower() in image_extensions for f in d.iterdir() if f.is_file()):
            class_dirs.append(d)

    if not class_dirs:
        log("No subdirectories with image files found")
        return

    # Create all class directories in train, val, and test splits to ensure consistent class indexing.
    all_class_names = {d.name for d in class_dirs}
    for class_name in all_class_names:
        (train_dest_path / class_name).mkdir(exist_ok=True)
        (val_dest_path / class_name).mkdir(exist_ok=True)
        if test_ratio > 0:
            (test_dest_path / class_name).mkdir(exist_ok=True)

    processed_class_names = set()

    # Process each class directory
    for class_dir in class_dirs:
        if cancel_event and cancel_event.is_set():
            log("Processing cancelled")
            return
        class_name = class_dir.name

        if class_name in processed_class_names:
            log(f"Warning: Class name '{class_name}' is duplicated Skipping to avoid data mixing")
            continue
        processed_class_names.add(class_name)

        # Collect image files.
        images = [f for f in class_dir.iterdir() if f.is_file() and f.suffix.lower() in image_extensions]
        num_images = len(images)

        if num_images == 0:
            continue

        # Report progress
        log(f'Found class: {class_name} with {num_images} images')

        # Shuffle images
        random.shuffle(images)

        # Split into train, val, and test sets.
        train_count = int(num_images * train_ratio)
        val_count = int(num_images * val_ratio)
        test_count = int(num_images * test_ratio)

        train_images = images[:train_count]
        val_images = images[train_count : train_count + val_count]
        test_images = images[train_count + val_count : train_count + val_count + test_count]
        
        # Create destination class directories and copy files
        if train_images:
            log(f'Copying {len(train_images)} training images for class {class_name}')
            for img in train_images:
                if cancel_event and cancel_event.is_set():
                    log("Processing cancelled")
                    return
                if resolution:
                    with Image.open(img) as image:
                        image = image.convert(color_mode).resize((resolution, resolution))
                        image.save(train_dest_path / class_name / img.name)
                else:
                    shutil.copy(img, train_dest_path / class_name / img.name)

        if val_images:
            log(f'Copying {len(val_images)} validation images for class {class_name}')
            for img in val_images:
                if cancel_event and cancel_event.is_set():
                    log("Processing cancelled")
                    return
                if resolution:
                    with Image.open(img) as image:
                        image = image.convert(color_mode).resize((resolution, resolution))
                        image.save(val_dest_path / class_name / img.name)
                else:
                    shutil.copy(img, val_dest_path / class_name / img.name)

        if test_images:
            log(f'Copying {len(test_images)} test images for class {class_name}')
            for img in test_images:
                if cancel_event and cancel_event.is_set():
                    log("Processing cancelled")
                    return
                if resolution:
                    with Image.open(img) as image:
                        image = image.convert(color_mode).resize((resolution, resolution))
                        image.save(test_dest_path / class_name / img.name)
                else:
                    shutil.copy(img, test_dest_path / class_name / img.name)

    # Final progress message
    log("Dataset processing complete")

if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='Process an image dataset into training, validation, and test sets')
    parser.add_argument('--source_dir', type=str, required=True, help='Source directory with class subfolders')
    parser.add_argument('--dest_dir', type=str, required=True, help='Destination directory for train/val/test splits')
    parser.add_argument('--train_ratio', type=float, default=0.8, help='Training set ratio')
    parser.add_argument('--val_ratio', type=float, default=0.1, help='Validation set ratio')
    parser.add_argument('--test_ratio', type=float, default=0.1, help='Test set ratio')
    parser.add_argument('--resolution', type=int, default=None, help='Resolution to resize images to')
    parser.add_argument('--seed', type=int, default=None, help='Random seed for reproducibility')
    parser.add_argument('--image_extensions', type=str, default='.jpg,.jpeg,.png', help='Comma-separated list of image file extensions to include')
    parser.add_argument('--color_mode', type=str, default='RGB', help='Target color mode for images (e.g., RGB, L)')
    parser.add_argument('--overwrite_dest', action='store_true', help='Overwrite destination directory if it exists')
    parser.add_argument('--train_dir_name', type=str, default='train', help='Name for the training directory')
    parser.add_argument('--val_dir_name', type=str, default='val', help='Name for the validation directory')
    parser.add_argument('--test_dir_name', type=str, default='test', help='Name for the test directory')
    
    parser.set_defaults(load_truncated_images=True)
    parser.add_argument('--no-load-truncated-images', dest='load_truncated_images', action='store_false', help='Do not attempt to load truncated images')

    args = parser.parse_args()

    # The process_dataset function now prints to the console by default.
    # A progress callback is not needed for command-line execution.
    process_dataset(args.source_dir, args.dest_dir, train_ratio=args.train_ratio, val_ratio=args.val_ratio, test_ratio=args.test_ratio, resolution=args.resolution, seed=args.seed, progress_callback=None, image_extensions=args.image_extensions, color_mode=args.color_mode, overwrite_dest=args.overwrite_dest, load_truncated_images=args.load_truncated_images, train_dir_name=args.train_dir_name, val_dir_name=args.val_dir_name, test_dir_name=args.test_dir_name)
